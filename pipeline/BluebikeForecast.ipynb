{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ei50RnIbAmX",
        "outputId": "9e923554-4ade-42d6-978f-94a2b1102220"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from datetime import timedelta, datetime\n",
        "import re\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_raw_data(csv_path):\n",
        "    df = pd.read_csv(csv_path,\n",
        "                    dtype={\"starttime\": \"string\", \"stoptime\": \"string\"},\n",
        "                    keep_default_na=False,)\n",
        "    return df"
      ],
      "metadata": {
        "id": "f5GnmtMKbIQh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_bike_time(series: pd.Series, freq: str = \"h\") -> pd.Series:\n",
        "    s = series.astype(str).str.strip().str.replace(\"\\u200b\", \"\", regex=False)\n",
        "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
        "    # fix single-digit hour \" 1:\" -> \" 01:\" if needed\n",
        "    m = dt.isna()\n",
        "    if m.any():\n",
        "        s2 = s[m].str.replace(r\" (\\d):\", lambda x: f\" 0{x.group(1)}:\", regex=True)\n",
        "        dt.loc[m] = pd.to_datetime(s2, format=\"%Y-%m-%d %H:%M:%S\", errors=\"coerce\")\n",
        "    return dt.dt.floor(freq)\n",
        "\n",
        "def normalize_station_id(s: pd.Series) -> pd.Series:\n",
        "    # handles numeric strings / floats cleanly; keeps <NA> if missing\n",
        "    return pd.to_numeric(s, errors=\"coerce\").astype(\"Int64\")"
      ],
      "metadata": {
        "id": "AdcNHMNUbLDr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # ---------- Normalize timestamps to the hour ----------\n",
        "    df[\"starttime\"] = parse_bike_time(df[\"starttime\"], freq=\"h\")\n",
        "    df[\"stoptime\"]  = parse_bike_time(df[\"stoptime\"],  freq=\"h\")\n",
        "\n",
        "    # ---------- Organize location data ----------\n",
        "    start_meta = (\n",
        "        df.rename(columns={\n",
        "            \"start station id\": \"station_id\",\n",
        "            \"start station name\": \"station_name\",\n",
        "            \"start station latitude\": \"latitude\",\n",
        "            \"start station longitude\": \"longitude\",\n",
        "        })[[\"station_id\", \"station_name\", \"latitude\", \"longitude\"]]\n",
        "    )\n",
        "    end_meta = (\n",
        "        df.rename(columns={\n",
        "            \"end station id\": \"station_id\",\n",
        "            \"end station name\": \"station_name\",\n",
        "            \"end station latitude\": \"latitude\",\n",
        "            \"end station longitude\": \"longitude\",\n",
        "        })[[\"station_id\", \"station_name\", \"latitude\", \"longitude\"]]\n",
        "    )\n",
        "\n",
        "    stations = pd.concat([start_meta, end_meta], ignore_index=True)\n",
        "    stations[\"station_id\"] = normalize_station_id(stations[\"station_id\"])\n",
        "    stations = stations.dropna(subset=[\"station_id\"]).drop_duplicates(\"station_id\")\n",
        "\n",
        "    # ---------- Count OUTS (undocks) by hour & start station ----------\n",
        "    outs = (\n",
        "        df.dropna(subset=[\"start station id\"])\n",
        "          .assign(station_id=lambda x: normalize_station_id(x[\"start station id\"]))\n",
        "          .groupby([\"starttime\", \"station_id\"], as_index=False)\n",
        "          .size()\n",
        "          .rename(columns={\"starttime\": \"timestart\", \"size\": \"out\"})\n",
        "    )\n",
        "\n",
        "    # ---------- Count INS (docks) by hour & end station ----------\n",
        "    ins = (\n",
        "        df.dropna(subset=[\"end station id\"])\n",
        "          .assign(station_id=lambda x: normalize_station_id(x[\"end station id\"]))\n",
        "          .groupby([\"stoptime\", \"station_id\"], as_index=False)\n",
        "          .size()\n",
        "          .rename(columns={\"stoptime\": \"timestart\", \"size\": \"in\"})\n",
        "    )\n",
        "\n",
        "    # ---------- Combine (outer join so hours with only in OR only out are kept) ----------\n",
        "    hourly = (\n",
        "        pd.merge(outs, ins, on=[\"timestart\", \"station_id\"], how=\"outer\")\n",
        "          .fillna({\"in\": 0, \"out\": 0})\n",
        "    )\n",
        "\n",
        "    # ---------- (Optional) Complete grid: every station × every hour ----------\n",
        "    start_hour = min(df[\"starttime\"].min(), df[\"stoptime\"].min())\n",
        "    end_hour   = max(df[\"starttime\"].max(), df[\"stoptime\"].max())\n",
        "    all_hours  = pd.date_range(start=start_hour, end=end_hour, freq=\"h\")\n",
        "\n",
        "    all_station_ids = (\n",
        "        stations[\"station_id\"]\n",
        "        .dropna()\n",
        "        .astype(\"int64\")\n",
        "        .unique()\n",
        "    )\n",
        "\n",
        "    full_idx = pd.MultiIndex.from_product(\n",
        "        [all_hours, all_station_ids],\n",
        "        names=[\"timestart\", \"station_id\"]\n",
        "    )\n",
        "\n",
        "    hourly = (\n",
        "        hourly.set_index([\"timestart\", \"station_id\"])\n",
        "              .reindex(full_idx, fill_value=0)\n",
        "              .reset_index()\n",
        "    )\n",
        "\n",
        "    hourly[\"in\"]  = hourly[\"in\"].astype(\"int64\")\n",
        "    hourly[\"out\"] = hourly[\"out\"].astype(\"int64\")\n",
        "\n",
        "    # ---------- Attach metadata & compute timeend ----------\n",
        "    hourly = (\n",
        "        hourly.merge(stations, on=\"station_id\", how=\"left\")\n",
        "              .assign(timeend=lambda x: x[\"timestart\"] + pd.Timedelta(hours=1))\n",
        "    )\n",
        "\n",
        "    # ---------- Add time-based features from the interval ----------\n",
        "    # Hour-of-week: Monday=0 ... Sunday=6, so 0–167\n",
        "    start_how = (hourly[\"timestart\"].dt.dayofweek * 24 + hourly[\"timestart\"].dt.hour).astype(\"int16\")\n",
        "    end_how   = (hourly[\"timeend\"].dt.dayofweek   * 24 + hourly[\"timeend\"].dt.hour).astype(\"int16\")\n",
        "    is_weekend = (hourly[\"timestart\"].dt.dayofweek >= 5).astype(\"int8\")  # Sat/Sun = 1\n",
        "    month_num  = hourly[\"timestart\"].dt.month.astype(\"int8\")             # 1–12\n",
        "\n",
        "    hourly = hourly.assign(\n",
        "        start_hour_of_week=start_how,\n",
        "        end_hour_of_week=end_how,\n",
        "        is_weekend=is_weekend,\n",
        "        month=month_num,\n",
        "    )\n",
        "\n",
        "    # ---------- Final column order ----------\n",
        "    hourly = (\n",
        "        hourly.loc[:, [\n",
        "            \"timestart\", \"timeend\",\n",
        "            \"station_name\",                 # ← moved here\n",
        "            \"start_hour_of_week\", \"end_hour_of_week\", \"is_weekend\", \"month\",\n",
        "            \"station_id\", \"latitude\", \"longitude\",\n",
        "            \"in\", \"out\"\n",
        "        ]]\n",
        "        .sort_values([\"timestart\", \"station_id\"])\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    return hourly\n"
      ],
      "metadata": {
        "id": "AV4UDjZbbMj_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pathlib import Path\n",
        "# from datetime import datetime\n",
        "# import pandas as pd\n",
        "\n",
        "# # ---------- Inputs ----------\n",
        "# input_dir  = Path(\"/content/drive/MyDrive/Side hustles/DS+X/Data/Raw\")\n",
        "# output_dir = Path(\"/content/drive/MyDrive/Side hustles/DS+X/Data/Transformed\")\n",
        "\n",
        "# files = [\n",
        "#     \"202205-bluebikes-tripdata\",\n",
        "#     \"202206-bluebikes-tripdata\",\n",
        "#     \"202207-bluebikes-tripdata\",\n",
        "# ]\n",
        "\n",
        "# paths = [input_dir / f\"{stem}.csv\" for stem in files]\n",
        "\n",
        "# # ---------- Load & concatenate ----------\n",
        "# # Assumes you already have load_raw_data(path) -> DataFrame\n",
        "# # If schemas differ slightly, pd.concat(..., sort=False) keeps all columns.\n",
        "# dfs = [load_raw_data(p) for p in paths]\n",
        "# df_all = pd.concat(dfs, ignore_index=True, sort=False)\n",
        "\n",
        "# # Optional sanity check\n",
        "# print(f\"Loaded {len(paths)} files; total rows: {len(df_all):,}\")\n",
        "\n",
        "# # ---------- Transform ----------\n",
        "# transformed_data = transform_data(df_all)\n",
        "\n",
        "# # ---------- Save (timestamped, safe filename) ----------\n",
        "# ts = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
        "# combo_tag = f\"{files[0]}_to_{files[-1]}\"  # e.g., 202205-..._to_202207-...\n",
        "# out_path = output_dir / f\"{combo_tag}_parsed_{ts}.csv\"\n",
        "\n",
        "# transformed_data.to_csv(out_path, index=False, date_format=\"%Y-%m-%d %H:%M:%S\")\n",
        "# print(f\"Saved: {out_path}\")\n"
      ],
      "metadata": {
        "id": "LfTXvVKrbNtI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TrsE0e35u_E",
        "outputId": "b9878670-0b7e-4172-a860-530b3e4ecc35"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# --- Load ---\n",
        "trainingPath = Path(\"/content/drive/MyDrive/Side hustles/DS+X/Data/Cleaned\")\n",
        "dtypes = {\n",
        "    \"stationid\": \"Int32\",\n",
        "    \"month\": \"Int16\",\n",
        "    \"weekhour\": \"Int16\",\n",
        "    \"TMAX\": \"float32\",\n",
        "    \"TMIN\": \"float32\",\n",
        "    \"PRCP\": \"float32\",\n",
        "}\n",
        "df = pd.read_csv(trainingPath / \"2023.csv\", dtype=dtypes, low_memory=False)\n",
        "\n",
        "# --- Features / Targets ---\n",
        "feature_cols = [\"stationid\", \"month\", \"weekhour\"]\n",
        "target_cols = [\"next_bike_in\", \"next_bike_out\"]\n",
        "\n",
        "# Ensure targets are numeric and drop rows with missing targets\n",
        "df[target_cols] = df[target_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
        "df = df.dropna(subset=target_cols).copy()\n",
        "\n",
        "X = df[feature_cols]\n",
        "y = df[target_cols]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=42, shuffle=True, stratify=df[\"stationid\"]\n",
        ")\n",
        "print(\"-----Training Data-----\")\n",
        "print(X_train.head(100))\n",
        "print(y_train.head(100))\n",
        "\n",
        "print(\"-----Testing Data-----\")\n",
        "print(X_test.head(100))\n",
        "print(y_test.head(100))\n",
        "\n",
        "# --- Model ---\n",
        "pre = \"passthrough\"  # <- no OHE, avoids missing column issues\n",
        "\n",
        "xgb = XGBRegressor(\n",
        "    objective=\"reg:squarederror\",\n",
        "    tree_method=\"hist\",\n",
        "    n_estimators=600,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=8,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    reg_lambda=1.0,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "model = Pipeline([\n",
        "    (\"prep\", pre),\n",
        "    (\"reg\", MultiOutputRegressor(xgb, n_jobs=-1)),\n",
        "])\n",
        "\n",
        "# --- Train ---\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# --- Evaluate ---\n",
        "pred = model.predict(X_test)\n",
        "pred_in, pred_out = pred[:, 0], pred[:, 1]\n",
        "\n",
        "mae_in  = mean_absolute_error(y_test[\"next_bike_in\"],  pred_in)\n",
        "mae_out = mean_absolute_error(y_test[\"next_bike_out\"], pred_out)\n",
        "rmse_in = np.sqrt(mean_squared_error(y_test[\"next_bike_in\"],  pred_in))\n",
        "rmse_out= np.sqrt(mean_squared_error(y_test[\"next_bike_out\"], pred_out))\n",
        "\n",
        "print(f\"MAE  in : {mae_in:.3f} | RMSE in : {rmse_in:.3f}\")\n",
        "print(f\"MAE out: {mae_out:.3f} | RMSE out: {rmse_out:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OM7V_1DBywIM",
        "outputId": "f43d99fc-bec7-42e9-c5a5-53e3b7e3aef9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Training Data-----\n",
            "         stationid  month  weekhour\n",
            "817927         114     10       112\n",
            "952626         136      1        69\n",
            "3175893        550      1       141\n",
            "306595          43      9        90\n",
            "2749116        485      1         7\n",
            "...            ...    ...       ...\n",
            "1211128        178      1        17\n",
            "2633993        468     12        71\n",
            "1272866        185      7        87\n",
            "1056972        150      4        85\n",
            "1875358        344     10       112\n",
            "\n",
            "[100 rows x 3 columns]\n",
            "         next_bike_in  next_bike_out\n",
            "817927            0.0            1.0\n",
            "952626            0.0            0.0\n",
            "3175893           0.0            0.0\n",
            "306595            0.0            0.0\n",
            "2749116           0.0            0.0\n",
            "...               ...            ...\n",
            "1211128           6.0            4.0\n",
            "2633993           0.0            0.0\n",
            "1272866           2.0            0.0\n",
            "1056972           0.0            1.0\n",
            "1875358           1.0            1.0\n",
            "\n",
            "[100 rows x 2 columns]\n",
            "-----Testing Data-----\n",
            "         stationid  month  weekhour\n",
            "3630342        637      6       106\n",
            "3171116        550      3        89\n",
            "861012         119      3       166\n",
            "1381605        199     10       141\n",
            "2740509        484      3        90\n",
            "...            ...    ...       ...\n",
            "311671          44      3        12\n",
            "2337813        409      4        95\n",
            "3667965        644      9       113\n",
            "3254600        562     12        18\n",
            "2023389        364      7        21\n",
            "\n",
            "[100 rows x 3 columns]\n",
            "         next_bike_in  next_bike_out\n",
            "3630342           0.0            0.0\n",
            "3171116           0.0            0.0\n",
            "861012            0.0            0.0\n",
            "1381605           0.0            1.0\n",
            "2740509           0.0            0.0\n",
            "...               ...            ...\n",
            "311671            0.0            0.0\n",
            "2337813           3.0            0.0\n",
            "3667965           0.0            0.0\n",
            "3254600           0.0            0.0\n",
            "2023389           2.0            0.0\n",
            "\n",
            "[100 rows x 2 columns]\n",
            "MAE  in : 1.033 | RMSE in : 1.913\n",
            "MAE out: 1.039 | RMSE out: 1.933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import dump, load\n",
        "\n",
        "keys = [\"stationid\", \"month\", \"weekhour\"]\n",
        "\n",
        "test_keys = df.loc[X_test.index, keys].reset_index(drop=True)\n",
        "\n",
        "compare = pd.DataFrame({\n",
        "    **{k: test_keys[k] for k in keys},\n",
        "    \"in_true\":   y_test[\"next_bike_in\"].to_numpy(),\n",
        "    \"in_pred\":   pred_in,\n",
        "    \"out_true\":   y_test[\"next_bike_out\"].to_numpy(),\n",
        "    \"out_pred\":   pred_out,\n",
        "})\n",
        "\n",
        "compare = compare.sort_values(keys).reset_index(drop=True)\n",
        "\n",
        "comparisonPath = Path(\"/content/drive/MyDrive/Side hustles/DS+X/Comparisons\")\n",
        "comparisonPath.mkdir(parents=True, exist_ok=True)\n",
        "comparisonOutputPath = comparisonPath / f\"bluebikes_xgb_predictions_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "compare.to_csv(comparisonOutputPath, index=False)\n",
        "print(f\"\\nSaved prediction comparisons to: {comparisonOutputPath}\")\n",
        "\n",
        "modelPath = Path(\"/content/drive/MyDrive/Side hustles/DS+X/Models\")\n",
        "modelPath.mkdir(parents=True, exist_ok=True)\n",
        "modelOutputPath = modelPath / \"bluebikes_xgb_multioutput.joblib\"\n",
        "dump(model, modelOutputPath)\n",
        "print(f\"Saved model to: {modelOutputPath}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofwka6oacPVp",
        "outputId": "51c9e5ef-45db-4578-fd8a-78111beea721"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved prediction comparisons to: /content/drive/MyDrive/Side hustles/DS+X/Comparisons/bluebikes_xgb_predictions_20251026_010932.csv\n",
            "Saved model to: /content/drive/MyDrive/Side hustles/DS+X/Models/bluebikes_xgb_multioutput.joblib\n"
          ]
        }
      ]
    }
  ]
}